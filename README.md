# aedist â€” AI-driven Energy Data Integration for Sustainable Transition

Benchmark and tools for evaluating AI systems on the production of economic statistics.
Case study: Vietnam thermal power plant inventory.

minh.ha-duong@cnrs.fr
2026-02-16

## Quick start

```bash
uv sync --extra dev

# Query 16 LLMs via OpenRouter (single-shot, ~$5)
export OPENROUTER_API_KEY=...
make query

# Extract CSV from JSON responses
make extract

# Evaluate all system outputs against reference
make evaluate
```

### Notes

- `make extract` reads the latest dated directory under `outputs/llm_direct/` (e.g. `YYYY-MM-DD/`) and writes one CSV per model into `outputs/llm_direct/`.
- Known issue (not fixed yet): two JSON files currently fail extraction because the embedded table is not parseable as a CSV with a recognizable header:
     - `outputs/llm_direct/2026-02-16/mistral-large-2512.json`
     - `outputs/llm_direct/2026-02-16/mistral-small-3.2-24b-instruct-2506.json`

## Pipeline

```
prompt + models.yaml
        â”‚
        â–¼
   query.py          16 models Ã— OpenRouter â†’ JSON (parallel, cached)
        â”‚
        â–¼
   extract.py        JSON responses â†’ CSV tables
        â”‚
        â–¼
   runner.py          CSV Ã— reference â†’ reconciliation + metrics JSON
        â”‚
        â–¼
   results/summary/all_metrics.json
```

## Repository structure

```
aedist/
â”œâ”€â”€ src/aedist/                 # Python package
â”‚   â”œâ”€â”€ schema.py               # Pydantic canonical data model (Plant, ReconciliationEntry)
â”‚   â”œâ”€â”€ cleaner/                # Config-driven normalization (names, provinces, fuels)
â”‚   â”œâ”€â”€ matching/               # MILP optimal assignment (lp.py) + greedy fallback
â”‚   â”œâ”€â”€ reconcile.py            # Global matching adapter (no provinceÃ—fuel grouping)
â”‚   â”œâ”€â”€ metrics.py              # Coverage, precision, F1, attribute accuracy, error taxonomy
â”‚   â”œâ”€â”€ runner.py               # CLI: aedist evaluate / evaluate-all
â”‚   â”œâ”€â”€ query.py                # Query LLMs via OpenRouter (parallel, daily cache)
â”‚   â””â”€â”€ convert.py              # Generate LaTeX tables from results
â”œâ”€â”€ data/reference/             # Expert-compiled datasets
â”‚   â”œâ”€â”€ vietnam_thermal_v1.csv  # Gold standard: 163 plants, canonical schema
â”‚   â”œâ”€â”€ vietnam_thermal_units_v1.csv  # Unit-level (251 units)
â”‚   â””â”€â”€ gem_thermal.csv         # Global Energy Monitor comparison
â”œâ”€â”€ outputs/                    # System outputs
â”‚   â”œâ”€â”€ llm_direct/             # Single-shot responses (JSON + CSV)
â”‚   â”œâ”€â”€ llm_multiturn/          # Multi-turn (relances)
â”‚   â”œâ”€â”€ rag_curated/            # RAG with curated corpus
â”‚   â””â”€â”€ rag_extended/           # RAG with extended corpus
â”œâ”€â”€ results/                    # Evaluation results
â”‚   â”œâ”€â”€ reconciliation/         # Per-run reconciliation tables
â”‚   â””â”€â”€ summary/                # all_metrics.json
â”œâ”€â”€ prompts/                    # Standardized prompts
â”œâ”€â”€ models.yaml                 # 16 models: US/CN/FR Ã— frontierâ†’edge Ã— open/commercial
â”œâ”€â”€ pdfOCR2md/                  # PDFâ†’Markdown conversion tool
â”œâ”€â”€ paper/                      # Benchmark paper source
â”œâ”€â”€ diagrams/                   # Architecture diagrams
â”œâ”€â”€ tests/                      # Integration + unit tests
â”œâ”€â”€ ADR.md                      # Architecture Decision Records
â”œâ”€â”€ TODO.md                     # Project roadmap
â”œâ”€â”€ Makefile                    # query, extract, evaluate
â””â”€â”€ pyproject.toml
```

## Model registry (16 models)

| Class | Models |
|-------|--------|
| Frontier | Claude Sonnet 4.5, Claude Opus 4.6, Gemini 3 Flash, Grok 4.1 Fast, DeepSeek V3.2 |
| Large | Mistral Large 3, Qwen3 235B, Llama 4 Maverick, MiniMax M2.5, Kimi K2.5 |
| Medium | Mistral Medium 3.1, Qwen3 32B, Nemotron 3 Nano 30B |
| Small/Edge | Mistral Small 3.2, Ministral 3 8B, Gemini 2.5 Flash Lite |

Coverage: ðŸ‡ºðŸ‡¸ 7 Â· ðŸ‡¨ðŸ‡³ 5 Â· ðŸ‡«ðŸ‡· 4 â€” Open 9 Â· Commercial 7

## Architecture decisions

See [ADR.md](ADR.md):

1. **Two repos**: Code+bench unified here; LaTeX report separate
2. **MILP matching**: Optimal global assignment via PuLP/CBC
3. **Global matching**: No provinceÃ—fuel grouping (attribute errors measured separately)
4. **Plant-level granularity**: 163 plants, not 251 units

## Key results (Claude 3.5 Sonnet, legacy data)

| Configuration | Matched | Coverage | Precision | F1 |
|---|---|---|---|---|
| Single-shot (concise) | 30/163 | 18.4% | 100% | 31.1% |
| Single-shot (normal) | 38/163 | 23.3% | 100% | 37.8% |
| Multi-turn (+1 relance) | 72/163 | 44.2% | 100% | 61.3% |
| RAG curated | 59/163 | 36.2% | 100% | 53.1% |
| RAG curated + 1 relance | 100/163 | 61.4% | 100% | 76.0% |

## License

CC-BY-SA 4.0
